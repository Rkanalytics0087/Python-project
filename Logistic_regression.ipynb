{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWTJKTaybQ1v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Theory Questions**"
      ],
      "metadata": {
        "id": "JLnZ39IhlioQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "Logistic Regression is a statistical method used for **binary classification**, predicting the probability that an observation belongs to a particular class (e.g., 0 or 1). Unlike Linear Regression, which predicts continuous values (e.g., house prices), Logistic Regression outputs probabilities that are constrained between 0 and 1, making it suitable for categorical outcomes.\n",
        "\n",
        "**Key Differences:**\n",
        "- **Output**: Linear Regression predicts a continuous value (e.g., y = 3.5), while Logistic Regression predicts a probability (e.g., P(y=1) = 0.75), which can be thresholded (e.g., >0.5) to classify.\n",
        "- **Function**: Linear Regression uses a straight line (y = mx + b), while Logistic Regression uses the sigmoid function to map predictions to probabilities.\n",
        "- **Purpose**: Linear Regression is for regression tasks; Logistic Regression is for classification tasks.\n",
        "- **Error Metric**: Linear Regression minimizes squared errors (e.g., MSE), while Logistic Regression minimizes a logarithmic loss (log-loss or cross-entropy).\n",
        "\n",
        "---\n",
        "\n",
        "### 2. What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "Logistic Regression models the probability of a binary outcome using the **sigmoid function**. The equation is:\n",
        "\n",
        "\\[ P(y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_nx_n)}} \\]\n",
        "\n",
        "Where:\n",
        "- \\( P(y=1|X) \\): Probability of the positive class given input features \\( X \\).\n",
        "- \\( \\beta_0 \\): Intercept (bias term).\n",
        "- \\( \\beta_1, \\beta_2, ..., \\beta_n \\): Coefficients (weights) for each feature \\( x_1, x_2, ..., x_n \\).\n",
        "- \\( e \\): Euler’s number (~2.718).\n",
        "\n",
        "The linear combination \\( \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n \\) is passed through the sigmoid function to produce a probability.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "The **sigmoid function** is used because it:\n",
        "- **Maps to (0, 1)**: It transforms any real-valued number (from \\(-\\infty\\) to \\(+\\infty\\)) into a range between 0 and 1, which is ideal for representing probabilities.\n",
        "- **Non-linear**: It introduces non-linearity, allowing the model to handle classification boundaries that aren’t just straight lines.\n",
        "- **Smooth Gradient**: It’s differentiable, which is essential for optimization using gradient descent to minimize the cost function.\n",
        "- **Thresholding**: Outputs can be interpreted with a threshold (e.g., 0.5) for binary classification.\n",
        "\n",
        "The sigmoid function is defined as:  \n",
        "\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\]\n",
        "\n",
        "---\n",
        "\n",
        "### 4. What is the cost function of Logistic Regression?\n",
        "\n",
        "The cost function for Logistic Regression is the **log-loss** (or **cross-entropy loss**), which measures the difference between predicted probabilities and actual labels. It’s defined as:\n",
        "\n",
        "\\[ J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)] \\]\n",
        "\n",
        "Where:\n",
        "- \\( m \\): Number of training examples.\n",
        "- \\( y_i \\): True label (0 or 1) for the \\(i\\)-th example.\n",
        "- \\( \\hat{y}_i \\): Predicted probability for the \\(i\\)-th example.\n",
        "- \\( \\beta \\): Model parameters to optimize.\n",
        "\n",
        "This function penalizes confident wrong predictions heavily (e.g., predicting 0.99 when the true label is 0) and is minimized using gradient descent.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "**Regularization** adds a penalty term to the cost function to prevent overfitting by discouraging overly complex models (large coefficients).\n",
        "\n",
        "- **How it works**: It modifies the cost function:  \n",
        "  \\[ J(\\beta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)] + \\lambda R(\\beta) \\]\n",
        "  Where \\( R(\\beta) \\) is the regularization term, and \\( \\lambda \\) controls its strength.\n",
        "- **Why it’s needed**:\n",
        "  - Prevents overfitting when the model fits noise in the training data.\n",
        "  - Improves generalization to unseen data.\n",
        "  - Handles multicollinearity (correlated features) by shrinking coefficients.\n",
        "\n",
        "Common types: L1 (Lasso), L2 (Ridge), and Elastic Net.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        "These are regularization techniques:\n",
        "- **Lasso (L1)**:\n",
        "  - Penalty: \\( R(\\beta) = \\sum |\\beta_j| \\) (absolute values of coefficients).\n",
        "  - Effect: Shrinks some coefficients to exactly 0, performing feature selection.\n",
        "  - Use: When you suspect only a few features are important.\n",
        "- **Ridge (L2)**:\n",
        "  - Penalty: \\( R(\\beta) = \\sum \\beta_j^2 \\) (squared coefficients).\n",
        "  - Effect: Shrinks coefficients toward 0 but not exactly 0, keeping all features.\n",
        "  - Use: When all features might contribute but need to be controlled.\n",
        "- **Elastic Net**:\n",
        "  - Penalty: Combines L1 and L2: \\( R(\\beta) = \\alpha \\sum |\\beta_j| + (1 - \\alpha) \\sum \\beta_j^2 \\).\n",
        "  - Effect: Balances feature selection (L1) and coefficient shrinkage (L2).\n",
        "  - Use: When features are correlated or you’re unsure which regularization to choose.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "Use **Elastic Net** when:\n",
        "- Features are **highly correlated** (Lasso might arbitrarily pick one; Ridge keeps all but doesn’t select).\n",
        "- You want a mix of **feature selection** (like Lasso) and **shrinkage** (like Ridge).\n",
        "- The dataset has **more features than observations** (Lasso alone struggles).\n",
        "- You’re unsure whether L1 or L2 is better—Elastic Net combines their strengths with a tunable parameter (\\( \\alpha \\)) to balance them.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. What is the impact of the regularization parameter \\( \\lambda \\) in Logistic Regression?\n",
        "\n",
        "The regularization parameter \\( \\lambda \\) (sometimes called \\( C^{-1} \\) in implementations like scikit-learn) controls the trade-off between fitting the data and keeping the model simple:\n",
        "- **Large \\( \\lambda \\)**:\n",
        "  - Strong regularization.\n",
        "  - Coefficients are heavily penalized, shrinking them closer to 0.\n",
        "  - Risk: Underfitting (model too simple, misses patterns).\n",
        "- **Small \\( \\lambda \\)**:\n",
        "  - Weak regularization.\n",
        "  - Coefficients can grow large, fitting the training data closely.\n",
        "  - Risk: Overfitting (model too complex, captures noise).\n",
        "- **\\( \\lambda = 0 \\)**: No regularization—pure Logistic Regression.\n",
        "\n",
        "Tuning \\( \\lambda \\) balances bias and variance for optimal performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 9. What are the key assumptions of Logistic Regression?\n",
        "\n",
        "Logistic Regression assumes:\n",
        "- **Binary Outcome**: The dependent variable is categorical (usually binary: 0 or 1).\n",
        "- **Independence**: Observations are independent of each other.\n",
        "- **Linearity in Log-Odds**: The log-odds of the outcome (not the outcome itself) are a linear combination of the features.\n",
        "- **No Multicollinearity**: Independent variables should not be too highly correlated (though regularization can help).\n",
        "- **Large Sample Size**: Reliable estimates require sufficient data, especially for rare events.\n",
        "- No strict assumption of normally distributed features (unlike Linear Regression).\n",
        "\n",
        "---\n",
        "\n",
        "### 10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "Alternatives include:\n",
        "- **Decision Trees**: Simple, interpretable, handles non-linear relationships.\n",
        "- **Random Forests**: Ensemble of trees, robust to overfitting, handles complex data.\n",
        "- **Support Vector Machines (SVM)**: Finds optimal hyperplanes, effective for high-dimensional data.\n",
        "- **Naive Bayes**: Probabilistic, fast, works well with text data.\n",
        "- **K-Nearest Neighbors (KNN)**: Non-parametric, based on proximity.\n",
        "- **Neural Networks**: Powerful for large datasets, captures complex patterns.\n",
        "- **Gradient Boosting (e.g., XGBoost)**: Ensemble method, high accuracy, handles imbalanced data.\n",
        "\n",
        "Choice depends on data size, complexity, and interpretability needs.\n",
        "\n",
        "---\n",
        "\n",
        "### 11. What are Classification Evaluation Metrics?\n",
        "\n",
        "Common metrics for evaluating classification models:\n",
        "- **Accuracy**: (TP + TN) / Total (fraction of correct predictions).\n",
        "- **Precision**: TP / (TP + FP) (accuracy of positive predictions).\n",
        "- **Recall (Sensitivity)**: TP / (TP + FN) (ability to find all positives).\n",
        "- **F1-Score**: 2 * (Precision * Recall) / (Precision + Recall) (harmonic mean of precision and recall).\n",
        "- **ROC-AUC**: Area under the Receiver Operating Characteristic curve (trade-off between true and false positive rates).\n",
        "- **Confusion Matrix**: Table showing TP, TN, FP, FN.\n",
        "- **Log-Loss**: Measures probability prediction quality.\n",
        "\n",
        "---\n",
        "\n",
        "### 12. How does class imbalance affect Logistic Regression?\n",
        "\n",
        "Class imbalance (e.g., 90% class 0, 10% class 1) affects Logistic Regression by:\n",
        "- **Bias Toward Majority Class**: The model may predict the majority class more often, ignoring the minority class.\n",
        "- **Misleading Accuracy**: High accuracy can mask poor performance on the minority class.\n",
        "- **Shifted Decision Boundary**: The model optimizes for overall error, skewing toward the majority.\n",
        "\n",
        "**Solutions**: Use class weights, oversample the minority class, undersample the majority, or use metrics like F1-score or AUC instead of accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### 13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "Hyperparameter tuning involves optimizing parameters not learned from the data (e.g., \\( \\lambda \\) or \\( C \\), penalty type) to improve model performance.\n",
        "- **Methods**:\n",
        "  - **Grid Search**: Tests all combinations in a specified range.\n",
        "  - **Random Search**: Samples random combinations, often faster.\n",
        "  - **Cross-Validation**: Evaluates performance on validation sets to avoid overfitting.\n",
        "- **Key Hyperparameters**: \\( C \\) (inverse of \\( \\lambda \\)), penalty (L1, L2, Elastic Net), solver.\n",
        "- **Goal**: Find the combination maximizing a metric (e.g., accuracy, F1-score).\n",
        "\n",
        "---\n",
        "\n",
        "### 14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "Solvers are optimization algorithms for minimizing the cost function:\n",
        "- **liblinear**: Uses coordinate descent, good for small datasets, supports L1 and L2.\n",
        "- **lbfgs**: Quasi-Newton method, efficient for medium datasets, L2 only.\n",
        "- **sag**: Stochastic average gradient, fast for large datasets, L2 only.\n",
        "- **saga**: Variant of sag, supports L1, L2, and Elastic Net, good for large datasets.\n",
        "- **newton-cg**: Newton’s method, computationally intensive, L2 only.\n",
        "\n",
        "**Which to use?**\n",
        "- Small dataset: liblinear.\n",
        "- Large dataset: saga (especially with Elastic Net).\n",
        "- Multiclass: lbfgs or saga.\n",
        "\n",
        "---\n",
        "\n",
        "### 15. How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "Logistic Regression extends to multiclass problems via:\n",
        "- **One-vs-Rest (OvR)**: Trains one binary classifier per class (class vs. all others), picks the highest probability.\n",
        "- **Softmax Regression**: Generalizes the sigmoid to multiple classes using the softmax function:  \n",
        "  \\[ P(y=k|X) = \\frac{e^{\\beta_k X}}{\\sum_{j} e^{\\beta_j X}} \\]  \n",
        "  Predicts probabilities across all classes simultaneously.\n",
        "\n",
        "---\n",
        "\n",
        "### 16. What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "**Advantages**:\n",
        "- Simple, interpretable, and fast to train.\n",
        "- Outputs probabilities, not just class labels.\n",
        "- Works well with linearly separable data.\n",
        "- Robust with small datasets.\n",
        "\n",
        "**Disadvantages**:\n",
        "- Assumes linearity in log-odds, struggles with non-linear relationships.\n",
        "- Sensitive to outliers and multicollinearity (without regularization).\n",
        "- Poor performance on complex, high-dimensional data compared to advanced models.\n",
        "\n",
        "---\n",
        "\n",
        "### 17. What are some use cases of Logistic Regression?\n",
        "\n",
        "- **Medical**: Predicting disease presence (e.g., cancer: yes/no).\n",
        "- **Marketing**: Customer churn prediction (e.g., will they leave?).\n",
        "- **Finance**: Credit risk assessment (e.g., default or not).\n",
        "- **Spam Detection**: Classifying emails (spam vs. not spam).\n",
        "- **Elections**: Predicting voter preference (e.g., candidate A vs. B).\n",
        "\n",
        "---\n",
        "\n",
        "### 18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "- **Logistic Regression**: For **binary classification**, uses the sigmoid function to predict one probability (P(y=1)).\n",
        "- **Softmax Regression**: For **multiclass classification**, uses the softmax function to predict probabilities across multiple classes, ensuring they sum to 1.\n",
        "\n",
        "Softmax is essentially Logistic Regression generalized to multiple classes.\n",
        "\n",
        "---\n",
        "\n",
        "### 19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "- **OvR**:\n",
        "  - Trains \\( k \\) separate binary classifiers (one per class).\n",
        "  - Simpler, computationally lighter for many classes.\n",
        "  - Use when: Classes are independent, or computational resources are limited.\n",
        "- **Softmax**:\n",
        "  - Trains one model, jointly optimizing all class probabilities.\n",
        "  - Captures inter-class relationships better.\n",
        "  - Use when: Classes are mutually exclusive, and you want a unified probability distribution.\n",
        "\n",
        "Choose based on problem complexity and resource constraints.\n",
        "\n",
        "---\n",
        "\n",
        "### 20. How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "Coefficients (\\( \\beta \\)) represent the change in **log-odds** for a one-unit increase in the feature, holding others constant:\n",
        "- **Positive \\( \\beta \\)**: Increases the log-odds (and probability) of the positive class.\n",
        "- **Negative \\( \\beta \\)**: Decreases the log-odds.\n",
        "- **Magnitude**: Larger absolute values indicate stronger influence.\n",
        "- **Exponentiation**: \\( e^{\\beta} \\) gives the odds ratio (e.g., \\( e^{0.5} = 1.65 \\) means odds increase by 65% per unit).\n",
        "\n"
      ],
      "metadata": {
        "id": "hYjkRnMVbfFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pratical Questions**"
      ],
      "metadata": {
        "id": "lgIojtUflblw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset (Iris as example)\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuGAklgFbgV-",
        "outputId": "935386c9-924e-4500-b72a-f9a7de781af1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty=\"l1\") and print the model accuracy.\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L1 (Lasso): {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7bhfuhLcInH",
        "outputId": "f836203c-89a9-43f8-a3b4-2a6cf2dbaa0e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L1 (Lasso): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty=\"l2\"). Print model accuracy and coefficients.\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train with L2 regularization\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with L2 (Ridge): {accuracy:.4f}\")\n",
        "print(\"Model Coefficients:\", model.coef_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zL54CIsh5LI",
        "outputId": "b69b3b07-d2d1-469a-a42d-b9886acffd4f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with L2 (Ridge): 1.0000\n",
            "Model Coefficients: [[-0.39340204  0.96258576 -2.37510761 -0.99874603]\n",
            " [ 0.50840364 -0.25486503 -0.21301366 -0.77575487]\n",
            " [-0.1150016  -0.70772072  2.58812127  1.77450091]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train with Elastic Net\n",
        "model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Elastic Net: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj-AKFy2iOln",
        "outputId": "2ff11a2e-6778-44a1-8bf8-968c8ce17916"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Elastic Net: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train with One-vs-Rest\n",
        "model = LogisticRegression(multi_class='ovr', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with OvR: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZhAUOiXiWRV",
        "outputId": "5ba8f1cb-2115-4b82-c7b3-bba990421b37"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with OvR: 0.9667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "model = LogisticRegression(max_iter=200)\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and accuracy\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-MrANa6JicR_",
        "outputId": "49437b01-c152-4ce2-b32c-b807d5c369a5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "Model Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "model = LogisticRegression(max_iter=200)\n",
        "accuracies = []\n",
        "\n",
        "# Cross-validation loop\n",
        "for train_idx, test_idx in skf.split(X, y):\n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = y[train_idx], y[test_idx]\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracies.append(accuracy_score(y_test, y_pred))\n",
        "\n",
        "print(f\"Average Accuracy: {np.mean(accuracies):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIBw-QXHiia-",
        "outputId": "6a8273fd-724b-4347-e7f7-c98286c5629b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.9667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset from CSV (replace with your file path)\n",
        "df = pd.read_csv('your_dataset.csv')  # Example: 'titanic.csv'\n",
        "X = df.drop('target_column', axis=1)  # Replace 'target_column' with your label\n",
        "y = df['target_column']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "FM3dWnr8ixtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_model import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter distribution\n",
        "param_dist = {'C': np.logspace(-3, 3, 10), 'penalty': ['l1', 'l2'], 'solver': ['liblinear', 'saga']}\n",
        "\n",
        "# Apply RandomizedSearchCV\n",
        "model = LogisticRegression(max_iter=200)\n",
        "random_search = RandomizedSearchCV(model, param_dist, n_iter=10, cv=5, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and accuracy\n",
        "y_pred = random_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Best Parameters: {random_search.best_params_}\")\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "eykJ2KH7jAtZ",
        "outputId": "c7395c15-9bae-402e-a91d-f8abfe31e5c3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sklearn.model_model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-9048c888313d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_iris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.model_model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train with One-vs-One\n",
        "model = OneVsOneClassifier(LogisticRegression(max_iter=200))\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with OvO: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGVbDluyjAxE",
        "outputId": "7672fe00-d3c3-4a0e-e7cf-ea47dc97843e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with OvO: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate binary dataset\n",
        "X, y = make_classification(n_classes=2, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and visualize confusion matrix\n",
        "y_pred = model.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "EA84wdKyjA0l",
        "outputId": "f5bcf855-f7c5-4687-dcd7-de6a309c8e71"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Generate binary dataset\n",
        "X, y = make_classification(n_classes=2, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4Ik40Gpjb2g",
        "outputId": "5cc6953f-1bcd-4a99-c9d0-d65e3ea7eeea"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0000\n",
            "Recall: 1.0000\n",
            "F1-Score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate imbalanced dataset\n",
        "X, y = make_classification(n_classes=2, weights=[0.9, 0.1], random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train with class weights\n",
        "model = LogisticRegression(class_weight='balanced', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with Class Weights: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1eMOrmFjfFV",
        "outputId": "fbd4d2d0-f1d0-4d77-bca8-9bfc4e77af8e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy with Class Weights: 0.8500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Titanic dataset (assumes you have it; download from Kaggle if needed)\n",
        "df = pd.read_csv('titanic.csv')  # Update path\n",
        "df = df[['Survived', 'Pclass', 'Sex', 'Age']].dropna()  # Simple subset\n",
        "df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})  # Encode Sex\n",
        "\n",
        "# Features and target\n",
        "X = df[['Pclass', 'Sex', 'Age']]\n",
        "y = df['Survived']\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "fLuuXynVjntQ",
        "outputId": "641e1d8c-28a8-41f6-e308-36f1ff3dc362"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'titanic.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-bd7f5f3b0d30>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load Titanic dataset (assumes you have it; download from Kaggle if needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'titanic.csv'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Update path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Survived'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Pclass'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sex'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Age'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Simple subset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sex'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sex'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'male'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'female'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Encode Sex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Without scaling\n",
        "model_no_scale = LogisticRegression(max_iter=200)\n",
        "model_no_scale.fit(X_train, y_train)\n",
        "y_pred_no_scale = model_no_scale.predict(X_test)\n",
        "acc_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "\n",
        "# With scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy without Scaling: {acc_no_scale:.4f}\")\n",
        "print(f\"Accuracy with Scaling: {acc_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSl_wpusjspg",
        "outputId": "f41300bd-a804-4cae-d0f0-35ebda2e374f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Scaling: 1.0000\n",
            "Accuracy with Scaling: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Generate binary dataset\n",
        "X, y = make_classification(n_classes=2, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities and evaluate ROC-AUC\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "id": "RU2HzZ0Uj8rR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train with custom C\n",
        "model = LogisticRegression(C=0.5, max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy with C=0.5: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "ARKX2uaIkDZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 18. Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importance (absolute coefficients)\n",
        "for name, coef in zip(feature_names, np.abs(model.coef_[0])):\n",
        "    print(f\"Feature: {name}, Coefficient Magnitude: {coef:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ8S_pxMkDb5",
        "outputId": "be3d9187-47fc-42ba-9ecb-496d1c6d03c3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature: sepal length (cm), Coefficient Magnitude: 0.3934\n",
            "Feature: sepal width (cm), Coefficient Magnitude: 0.9626\n",
            "Feature: petal length (cm), Coefficient Magnitude: 2.3751\n",
            "Feature: petal width (cm), Coefficient Magnitude: 0.9987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 19. Write a Python program to train Logistic Regression and evaluate its performance using Cohen's Kappa Score.\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "# Generate binary dataset\n",
        "X, y = make_classification(n_classes=2, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0negOglEkDgG",
        "outputId": "257cad6d-8db3-4c7a-c6aa-9d660ead22d3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate binary dataset\n",
        "X, y = make_classification(n_classes=2, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities and plot PR curve\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "bXvY_halkDiV",
        "outputId": "8f8bd6c0-ba93-48b8-b9db-83700fecd31b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_model import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Test solvers\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "for solver in solvers:\n",
        "    model = LogisticRegression(solver=solver, max_iter=200)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Accuracy with {solver}: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "1ZxEOMWWkhnI",
        "outputId": "7a33e21b-c9af-47c5-ef44-89e5963fe210"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sklearn.model_model'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-7855a7cbb370>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_iris\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.model_model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Generate binary dataset\n",
        "X, y = make_classification(n_classes=2, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LHKCayWkidO",
        "outputId": "dd018ead-dc57-48e8-acc0-40c34ff5a7ed"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Raw data\n",
        "model_raw = LogisticRegression(max_iter=200)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "acc_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# Standardized data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "model_scaled = LogisticRegression(max_iter=200)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy on Raw Data: {acc_raw:.4f}\")\n",
        "print(f\"Accuracy on Standardized Data: {acc_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wp2Irx-5k1JH",
        "outputId": "2ea65790-080d-47ac-9662-9b222434fa54"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Raw Data: 1.0000\n",
            "Accuracy on Standardized Data: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Grid search for optimal C\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "model = LogisticRegression(max_iter=200)\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = grid_search.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Optimal C: {grid_search.best_params_['C']}\")\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kqxVXT8k6O_",
        "outputId": "41118ad0-9e56-4148-a787-0432277f6729"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal C: 1\n",
            "Model Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and save model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "joblib.dump(model, 'logistic_model.pkl')\n",
        "\n",
        "# Load model and predict\n",
        "loaded_model = joblib.load('logistic_model.pkl')\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy after Loading: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ihcxpbSk_-h",
        "outputId": "d19b5867-b351-444d-923c-fd4f6ffbe972"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy after Loading: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KF8tbLDslIik"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}